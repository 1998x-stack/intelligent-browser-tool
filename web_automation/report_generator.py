#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ¥å‘Šç”Ÿæˆå™¨ - Report Generator

ç”Ÿæˆçˆ¬å–ç»“æœçš„MarkdownæŠ¥å‘Š,åŒ…å«ç»Ÿè®¡æ‘˜è¦ã€é¡µé¢åˆ†æã€å‘ç°æ•°æ®ç­‰ã€‚
Generates comprehensive Markdown reports from crawl results including
statistics, page analysis, and extracted data.

è®¾è®¡åŸåˆ™ (Design Principles):
- CleanRLå“²å­¦: å•æ–‡ä»¶è‡ªåŒ…å«ã€é€æ˜å¤„ç†æµç¨‹ã€æœ€å°åŒ–æŠ½è±¡ã€ä¾¿äºè°ƒè¯•
- æ¨¡æ¿åŒ–è¾“å‡º: ä½¿ç”¨å­—ç¬¦ä¸²æ¨¡æ¿ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š
- å¤šè¯­è¨€æ”¯æŒ: ä¸­è‹±æ–‡åŒè¯­æŠ¥å‘Šå†…å®¹

Author: AI Assistant
Date: 2024
"""

import sys
import json
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any
from datetime import datetime
from pathlib import Path
from loguru import logger

# ============================================================================
# é”™è¯¯å¤„ç† (Error Handling)
# ============================================================================

def get_err_message() -> str:
    """è·å–å½“å‰å¼‚å¸¸çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯"""
    exc_type, exc_value, exc_tb = sys.exc_info()
    if exc_type is None:
        return "No exception"
    return f"{exc_type.__name__}: {exc_value} (line {exc_tb.tb_lineno})"


# ============================================================================
# æ•°æ®ç»“æ„ (Data Structures)
# ============================================================================

@dataclass
class CrawlSummary:
    """
    çˆ¬å–æ‘˜è¦ç»Ÿè®¡
    
    Attributes:
        start_time: çˆ¬å–å¼€å§‹æ—¶é—´
        end_time: çˆ¬å–ç»“æŸæ—¶é—´
        total_pages: æ€»é¡µé¢æ•°
        successful_pages: æˆåŠŸé¡µé¢æ•°
        failed_pages: å¤±è´¥é¡µé¢æ•°
        total_urls_found: å‘ç°çš„URLæ€»æ•°
        total_data_extracted: æå–çš„æ•°æ®é¡¹æ•°
        intent: ç”¨æˆ·æ„å›¾
        start_url: èµ·å§‹URL
    """
    start_time: datetime
    end_time: Optional[datetime] = None
    total_pages: int = 0
    successful_pages: int = 0
    failed_pages: int = 0
    total_urls_found: int = 0
    total_data_extracted: int = 0
    intent: str = ""
    start_url: str = ""


@dataclass
class PageReport:
    """
    å•é¡µé¢æŠ¥å‘Š
    
    Attributes:
        url: é¡µé¢URL
        title: é¡µé¢æ ‡é¢˜
        relevance_score: ç›¸å…³æ€§åˆ†æ•° (0-1)
        key_findings: å…³é”®å‘ç°åˆ—è¡¨
        extracted_data: æå–çš„æ•°æ®
        summary: å†…å®¹æ‘˜è¦
        priority_urls: ä¼˜å…ˆURLåˆ—è¡¨
        fetch_time: æŠ“å–è€—æ—¶
        analysis_time: åˆ†æè€—æ—¶
        success: æ˜¯å¦æˆåŠŸ
        error: é”™è¯¯ä¿¡æ¯
    """
    url: str
    title: str = ""
    relevance_score: float = 0.0
    key_findings: List[str] = field(default_factory=list)
    extracted_data: Dict[str, Any] = field(default_factory=dict)
    summary: str = ""
    priority_urls: List[Dict] = field(default_factory=list)
    fetch_time: float = 0.0
    analysis_time: float = 0.0
    success: bool = True
    error: Optional[str] = None


# ============================================================================
# æŠ¥å‘Šæ¨¡æ¿ (Report Templates)
# ============================================================================

REPORT_HEADER_TEMPLATE = """# ç½‘é¡µçˆ¬å–åˆ†ææŠ¥å‘Š
# Web Crawling Analysis Report

---

## åŸºæœ¬ä¿¡æ¯ / Basic Information

| é¡¹ç›® / Item | å€¼ / Value |
|------------|-----------|
| **èµ·å§‹URL / Start URL** | {start_url} |
| **ç”¨æˆ·æ„å›¾ / Intent** | {intent} |
| **å¼€å§‹æ—¶é—´ / Start Time** | {start_time} |
| **ç»“æŸæ—¶é—´ / End Time** | {end_time} |
| **æ€»è€—æ—¶ / Duration** | {duration} |

---

## ç»Ÿè®¡æ‘˜è¦ / Statistics Summary

| æŒ‡æ ‡ / Metric | æ•°å€¼ / Value |
|--------------|-------------|
| æ€»é¡µé¢æ•° / Total Pages | {total_pages} |
| æˆåŠŸé¡µé¢ / Successful | {successful_pages} |
| å¤±è´¥é¡µé¢ / Failed | {failed_pages} |
| æˆåŠŸç‡ / Success Rate | {success_rate}% |
| å‘ç°URLæ•° / URLs Found | {total_urls_found} |
| æå–æ•°æ®é¡¹ / Data Items | {total_data_extracted} |

---

"""

PAGE_REPORT_TEMPLATE = """### ğŸ“„ {title}

**URL**: {url}

**ç›¸å…³æ€§åˆ†æ•° / Relevance Score**: {relevance_score:.2f}

{findings_section}

{data_section}

{summary_section}

{urls_section}

**å¤„ç†æ—¶é—´ / Processing Time**: æŠ“å– {fetch_time:.2f}s, åˆ†æ {analysis_time:.2f}s

---

"""

ERROR_PAGE_TEMPLATE = """### âŒ å¤„ç†å¤±è´¥ / Failed

**URL**: {url}

**é”™è¯¯ä¿¡æ¯ / Error**: {error}

---

"""

# ============================================================================
# æŠ¥å‘Šç”Ÿæˆå™¨ (Report Generator)
# ============================================================================

class ReportGenerator:
    """
    MarkdownæŠ¥å‘Šç”Ÿæˆå™¨
    
    Features:
        - ç»“æ„åŒ–æŠ¥å‘Š: ç»Ÿè®¡æ‘˜è¦ã€é¡µé¢åˆ†æã€æ•°æ®æå–
        - å¤šæ ¼å¼è¾“å‡º: Markdownã€JSON
        - ä¸­è‹±æ–‡åŒè¯­: æ”¯æŒä¸­è‹±æ–‡æ ‡ç­¾
    
    Example:
        >>> generator = ReportGenerator()
        >>> generator.set_summary(summary)
        >>> generator.add_page_report(page_report)
        >>> report = generator.generate()
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨"""
        self.summary: Optional[CrawlSummary] = None
        self.page_reports: List[PageReport] = []
        self.metadata: Dict[str, Any] = {}
        
        logger.info("ReportGenerator initialized")
    
    def set_summary(self, summary: CrawlSummary) -> None:
        """è®¾ç½®çˆ¬å–æ‘˜è¦"""
        self.summary = summary
        logger.debug(f"Summary set: {summary.total_pages} pages")
    
    def add_page_report(self, report: PageReport) -> None:
        """æ·»åŠ é¡µé¢æŠ¥å‘Š"""
        self.page_reports.append(report)
        logger.debug(f"Page report added: {report.url}")
    
    def add_metadata(self, key: str, value: Any) -> None:
        """æ·»åŠ å…ƒæ•°æ®"""
        self.metadata[key] = value
    
    # ========================================================================
    # æ ¼å¼åŒ–è¾…åŠ©æ–¹æ³• (Formatting Helpers)
    # ========================================================================
    
    def _format_duration(self, start: datetime, end: Optional[datetime]) -> str:
        """æ ¼å¼åŒ–æ—¶é—´é—´éš”"""
        if not end:
            return "è¿›è¡Œä¸­ / In Progress"
        
        delta = end - start
        total_seconds = int(delta.total_seconds())
        
        if total_seconds < 60:
            return f"{total_seconds}ç§’ / {total_seconds}s"
        elif total_seconds < 3600:
            minutes = total_seconds // 60
            seconds = total_seconds % 60
            return f"{minutes}åˆ†{seconds}ç§’ / {minutes}m {seconds}s"
        else:
            hours = total_seconds // 3600
            minutes = (total_seconds % 3600) // 60
            return f"{hours}å°æ—¶{minutes}åˆ† / {hours}h {minutes}m"
    
    def _format_findings(self, findings: List[str]) -> str:
        """æ ¼å¼åŒ–å…³é”®å‘ç°"""
        if not findings:
            return ""
        
        lines = ["**å…³é”®å‘ç° / Key Findings**:", ""]
        for i, finding in enumerate(findings, 1):
            lines.append(f"{i}. {finding}")
        lines.append("")
        
        return "\n".join(lines)
    
    def _format_extracted_data(self, data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æå–çš„æ•°æ®"""
        if not data:
            return ""
        
        lines = ["**æå–æ•°æ® / Extracted Data**:", ""]
        
        for key, value in data.items():
            if isinstance(value, list):
                lines.append(f"- **{key}**:")
                for item in value[:5]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                    lines.append(f"  - {item}")
                if len(value) > 5:
                    lines.append(f"  - ... ({len(value) - 5} more)")
            elif isinstance(value, dict):
                lines.append(f"- **{key}**: {json.dumps(value, ensure_ascii=False)[:100]}...")
            else:
                lines.append(f"- **{key}**: {value}")
        
        lines.append("")
        return "\n".join(lines)
    
    def _format_summary(self, summary: str) -> str:
        """æ ¼å¼åŒ–å†…å®¹æ‘˜è¦"""
        if not summary:
            return ""
        
        return f"**æ‘˜è¦ / Summary**:\n\n> {summary}\n"
    
    def _format_priority_urls(self, urls: List[Dict]) -> str:
        """æ ¼å¼åŒ–ä¼˜å…ˆURLåˆ—è¡¨"""
        if not urls:
            return ""
        
        lines = ["**æ¨èè®¿é—® / Recommended URLs**:", ""]
        
        priority_labels = {
            1: "ğŸ”´ é«˜ / High",
            2: "ğŸŸ¡ ä¸­ / Medium",
            3: "ğŸŸ¢ ä½ / Low"
        }
        
        for url_info in urls[:10]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
            url = url_info.get('url', '')
            priority = url_info.get('priority', 2)
            reason = url_info.get('reason', '')
            
            priority_label = priority_labels.get(priority, "ä¸­ / Medium")
            lines.append(f"- [{priority_label}] {url}")
            if reason:
                lines.append(f"  - åŸå› : {reason}")
        
        if len(urls) > 10:
            lines.append(f"- ... ({len(urls) - 10} more URLs)")
        
        lines.append("")
        return "\n".join(lines)
    
    # ========================================================================
    # æŠ¥å‘Šç”Ÿæˆ (Report Generation)
    # ========================================================================
    
    def _generate_header(self) -> str:
        """ç”ŸæˆæŠ¥å‘Šå¤´éƒ¨"""
        if not self.summary:
            return "# Web Crawling Report\n\n*No summary available*\n\n"
        
        s = self.summary
        success_rate = (s.successful_pages / s.total_pages * 100) if s.total_pages > 0 else 0
        
        return REPORT_HEADER_TEMPLATE.format(
            start_url=s.start_url,
            intent=s.intent,
            start_time=s.start_time.strftime("%Y-%m-%d %H:%M:%S"),
            end_time=s.end_time.strftime("%Y-%m-%d %H:%M:%S") if s.end_time else "N/A",
            duration=self._format_duration(s.start_time, s.end_time),
            total_pages=s.total_pages,
            successful_pages=s.successful_pages,
            failed_pages=s.failed_pages,
            success_rate=f"{success_rate:.1f}",
            total_urls_found=s.total_urls_found,
            total_data_extracted=s.total_data_extracted
        )
    
    def _generate_page_section(self, report: PageReport) -> str:
        """ç”Ÿæˆå•é¡µé¢æŠ¥å‘Šsection"""
        if not report.success:
            return ERROR_PAGE_TEMPLATE.format(
                url=report.url,
                error=report.error or "Unknown error"
            )
        
        return PAGE_REPORT_TEMPLATE.format(
            title=report.title or "Untitled",
            url=report.url,
            relevance_score=report.relevance_score,
            findings_section=self._format_findings(report.key_findings),
            data_section=self._format_extracted_data(report.extracted_data),
            summary_section=self._format_summary(report.summary),
            urls_section=self._format_priority_urls(report.priority_urls),
            fetch_time=report.fetch_time,
            analysis_time=report.analysis_time
        )
    
    def _generate_pages_section(self) -> str:
        """ç”Ÿæˆæ‰€æœ‰é¡µé¢æŠ¥å‘Š"""
        if not self.page_reports:
            return "## é¡µé¢åˆ†æ / Page Analysis\n\n*No pages analyzed*\n\n"
        
        lines = ["## é¡µé¢åˆ†æ / Page Analysis", ""]
        
        # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
        sorted_reports = sorted(
            self.page_reports,
            key=lambda x: x.relevance_score,
            reverse=True
        )
        
        for report in sorted_reports:
            lines.append(self._generate_page_section(report))
        
        return "\n".join(lines)
    
    def _generate_data_summary(self) -> str:
        """ç”Ÿæˆæ•°æ®æ±‡æ€»section"""
        all_data = {}
        all_findings = []
        
        for report in self.page_reports:
            if report.success:
                # æ”¶é›†æ‰€æœ‰å‘ç°
                all_findings.extend(report.key_findings)
                
                # åˆå¹¶æå–çš„æ•°æ®
                for key, value in report.extracted_data.items():
                    if key not in all_data:
                        all_data[key] = []
                    if isinstance(value, list):
                        all_data[key].extend(value)
                    else:
                        all_data[key].append(value)
        
        lines = ["## æ•°æ®æ±‡æ€» / Data Summary", ""]
        
        # å…³é”®å‘ç°æ±‡æ€»
        if all_findings:
            lines.append("### æ‰€æœ‰å…³é”®å‘ç° / All Key Findings")
            lines.append("")
            for i, finding in enumerate(all_findings[:20], 1):
                lines.append(f"{i}. {finding}")
            if len(all_findings) > 20:
                lines.append(f"\n*... and {len(all_findings) - 20} more findings*")
            lines.append("")
        
        # æå–æ•°æ®æ±‡æ€»
        if all_data:
            lines.append("### æå–æ•°æ®æ±‡æ€» / Extracted Data Summary")
            lines.append("")
            for key, values in all_data.items():
                unique_values = list(set(str(v) for v in values if v))[:10]
                lines.append(f"**{key}** ({len(values)} items):")
                for v in unique_values:
                    lines.append(f"  - {v[:100]}{'...' if len(v) > 100 else ''}")
                lines.append("")
        
        lines.append("---")
        lines.append("")
        
        return "\n".join(lines)
    
    def _generate_footer(self) -> str:
        """ç”ŸæˆæŠ¥å‘Šåº•éƒ¨"""
        lines = [
            "## æŠ¥å‘Šä¿¡æ¯ / Report Information",
            "",
            f"- **ç”Ÿæˆæ—¶é—´ / Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"- **æŠ¥å‘Šç‰ˆæœ¬ / Version**: 1.0",
            f"- **ç”Ÿæˆå™¨ / Generator**: Web Automation Tool",
            ""
        ]
        
        if self.metadata:
            lines.append("### å…ƒæ•°æ® / Metadata")
            lines.append("")
            for key, value in self.metadata.items():
                lines.append(f"- **{key}**: {value}")
            lines.append("")
        
        lines.extend([
            "---",
            "",
            "*æ­¤æŠ¥å‘Šç”±è‡ªåŠ¨åŒ–å·¥å…·ç”Ÿæˆ / This report was generated automatically*"
        ])
        
        return "\n".join(lines)
    
    def generate(self) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„MarkdownæŠ¥å‘Š
        
        Returns:
            Markdownæ ¼å¼çš„æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        try:
            sections = [
                self._generate_header(),
                self._generate_pages_section(),
                self._generate_data_summary(),
                self._generate_footer()
            ]
            
            report = "\n".join(sections)
            logger.info(f"Report generated: {len(report)} characters")
            
            return report
            
        except Exception:
            logger.error(f"Report generation failed: {get_err_message()}")
            return f"# Error\n\nFailed to generate report: {get_err_message()}"
    
    def generate_json(self) -> Dict[str, Any]:
        """
        ç”ŸæˆJSONæ ¼å¼çš„æŠ¥å‘Šæ•°æ®
        
        Returns:
            æŠ¥å‘Šæ•°æ®å­—å…¸
        """
        try:
            data = {
                'generated_at': datetime.now().isoformat(),
                'summary': None,
                'pages': [],
                'metadata': self.metadata
            }
            
            if self.summary:
                s = self.summary
                data['summary'] = {
                    'start_url': s.start_url,
                    'intent': s.intent,
                    'start_time': s.start_time.isoformat(),
                    'end_time': s.end_time.isoformat() if s.end_time else None,
                    'total_pages': s.total_pages,
                    'successful_pages': s.successful_pages,
                    'failed_pages': s.failed_pages,
                    'total_urls_found': s.total_urls_found,
                    'total_data_extracted': s.total_data_extracted
                }
            
            for report in self.page_reports:
                data['pages'].append({
                    'url': report.url,
                    'title': report.title,
                    'relevance_score': report.relevance_score,
                    'key_findings': report.key_findings,
                    'extracted_data': report.extracted_data,
                    'summary': report.summary,
                    'priority_urls': report.priority_urls,
                    'fetch_time': report.fetch_time,
                    'analysis_time': report.analysis_time,
                    'success': report.success,
                    'error': report.error
                })
            
            logger.info("JSON report generated")
            return data
            
        except Exception:
            logger.error(f"JSON report generation failed: {get_err_message()}")
            return {'error': get_err_message()}
    
    def save_report(
        self,
        output_dir: str,
        filename: Optional[str] = None,
        formats: List[str] = None
    ) -> Dict[str, str]:
        """
        ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            filename: æ–‡ä»¶å (ä¸å«æ‰©å±•å)
            formats: è¾“å‡ºæ ¼å¼åˆ—è¡¨ ['md', 'json']
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        formats = formats or ['md']
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"crawl_report_{timestamp}"
        
        saved_files = {}
        
        try:
            if 'md' in formats:
                md_path = output_path / f"{filename}.md"
                md_content = self.generate()
                md_path.write_text(md_content, encoding='utf-8')
                saved_files['md'] = str(md_path)
                logger.info(f"Markdown report saved: {md_path}")
            
            if 'json' in formats:
                json_path = output_path / f"{filename}.json"
                json_data = self.generate_json()
                json_path.write_text(
                    json.dumps(json_data, ensure_ascii=False, indent=2),
                    encoding='utf-8'
                )
                saved_files['json'] = str(json_path)
                logger.info(f"JSON report saved: {json_path}")
            
            return saved_files
            
        except Exception:
            logger.error(f"Failed to save report: {get_err_message()}")
            return saved_files
    
    def reset(self) -> None:
        """é‡ç½®æŠ¥å‘Šç”Ÿæˆå™¨"""
        self.summary = None
        self.page_reports = []
        self.metadata = {}
        logger.debug("Report generator reset")


# ============================================================================
# ä¾¿æ·å‡½æ•° (Convenience Functions)
# ============================================================================

def create_summary_from_results(
    results: List[Dict],
    start_url: str,
    intent: str,
    start_time: datetime,
    end_time: Optional[datetime] = None
) -> CrawlSummary:
    """
    ä»ç»“æœåˆ—è¡¨åˆ›å»ºæ‘˜è¦
    
    Args:
        results: é¡µé¢ç»“æœåˆ—è¡¨
        start_url: èµ·å§‹URL
        intent: ç”¨æˆ·æ„å›¾
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        
    Returns:
        CrawlSummaryå¯¹è±¡
    """
    successful = sum(1 for r in results if r.get('success', True))
    failed = len(results) - successful
    
    total_urls = sum(len(r.get('priority_urls', [])) for r in results)
    total_data = sum(len(r.get('extracted_data', {})) for r in results)
    
    return CrawlSummary(
        start_time=start_time,
        end_time=end_time or datetime.now(),
        total_pages=len(results),
        successful_pages=successful,
        failed_pages=failed,
        total_urls_found=total_urls,
        total_data_extracted=total_data,
        intent=intent,
        start_url=start_url
    )


def create_page_report_from_result(result: Dict) -> PageReport:
    """
    ä»ç»“æœå­—å…¸åˆ›å»ºé¡µé¢æŠ¥å‘Š
    
    Args:
        result: é¡µé¢ç»“æœå­—å…¸
        
    Returns:
        PageReportå¯¹è±¡
    """
    return PageReport(
        url=result.get('url', ''),
        title=result.get('title', ''),
        relevance_score=result.get('relevance_score', 0.0),
        key_findings=result.get('key_findings', []),
        extracted_data=result.get('extracted_data', {}),
        summary=result.get('summary', ''),
        priority_urls=result.get('priority_urls', []),
        fetch_time=result.get('fetch_time', 0.0),
        analysis_time=result.get('analysis_time', 0.0),
        success=result.get('success', True),
        error=result.get('error')
    )


# ============================================================================
# æµ‹è¯•ä»£ç  (Test Code)
# ============================================================================

if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logger.remove()
    logger.add(
        sys.stderr,
        level="DEBUG",
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}"
    )
    
    print("=" * 60)
    print("Report Generator Test")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    start_time = datetime.now()
    
    # åˆ›å»ºæ‘˜è¦
    summary = CrawlSummary(
        start_time=start_time,
        end_time=datetime.now(),
        total_pages=3,
        successful_pages=2,
        failed_pages=1,
        total_urls_found=15,
        total_data_extracted=8,
        intent="æ‹›ç”Ÿä¿¡æ¯",
        start_url="https://www.stanford.edu/"
    )
    
    # åˆ›å»ºé¡µé¢æŠ¥å‘Š
    page1 = PageReport(
        url="https://www.stanford.edu/admission",
        title="Stanford Admission",
        relevance_score=0.95,
        key_findings=[
            "æœ¬ç§‘ç”³è¯·æˆªæ­¢æ—¥æœŸä¸º1æœˆ2æ—¥",
            "éœ€è¦æäº¤SAT/ACTæˆç»©",
            "å½•å–ç‡çº¦ä¸º4%"
        ],
        extracted_data={
            "deadline": "January 2",
            "acceptance_rate": "4%",
            "required_tests": ["SAT", "ACT"]
        },
        summary="æ–¯å¦ç¦å¤§å­¦æ‹›ç”Ÿé¡µé¢,åŒ…å«æœ¬ç§‘å’Œç ”ç©¶ç”Ÿç”³è¯·ä¿¡æ¯ã€‚",
        priority_urls=[
            {"url": "https://www.stanford.edu/apply", "priority": 1, "reason": "ç”³è¯·å…¥å£"},
            {"url": "https://www.stanford.edu/finaid", "priority": 2, "reason": "ç»æµæ´åŠ©"}
        ],
        fetch_time=2.5,
        analysis_time=3.2,
        success=True
    )
    
    page2 = PageReport(
        url="https://www.stanford.edu/about",
        title="About Stanford",
        relevance_score=0.45,
        key_findings=["å­¦æ ¡æˆç«‹äº1885å¹´", "ä½äºåŠ å·å¸•æ´›é˜¿å°”æ‰˜"],
        extracted_data={"founded": "1885", "location": "Palo Alto, CA"},
        summary="å­¦æ ¡ç®€ä»‹é¡µé¢",
        priority_urls=[],
        fetch_time=1.8,
        analysis_time=2.1,
        success=True
    )
    
    page3 = PageReport(
        url="https://www.stanford.edu/broken",
        title="",
        success=False,
        error="Connection timeout"
    )
    
    # ç”ŸæˆæŠ¥å‘Š
    generator = ReportGenerator()
    generator.set_summary(summary)
    generator.add_page_report(page1)
    generator.add_page_report(page2)
    generator.add_page_report(page3)
    generator.add_metadata("crawler_version", "1.0.0")
    generator.add_metadata("user_agent", "WebAutomationBot/1.0")
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    report = generator.generate()
    print("\n" + "=" * 60)
    print("Generated Markdown Report:")
    print("=" * 60)
    print(report)
    
    # ä¿å­˜æŠ¥å‘Š
    saved = generator.save_report(
        output_dir="/home/claude/web_automation/test_reports",
        filename="test_report",
        formats=['md', 'json']
    )
    print(f"\nSaved files: {saved}")
    
    print("\næµ‹è¯•å®Œæˆ!")
